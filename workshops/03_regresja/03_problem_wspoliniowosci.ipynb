{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"9AQrOdajQbU_"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","\n","import statsmodels.api as sm\n","import statsmodels.formula.api as smf\n","\n","from scipy import stats\n","from patsy import dmatrices\n","\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"Rh2so0xcQbVB"},"source":["# Problem współliniowości"]},{"cell_type":"markdown","metadata":{"id":"AsR3vrUUQbVD"},"source":["### Diagnostyka\n","\n","1. **Macierz korelacji predyktorów** - $D_X = (\\rho_{ij})$;\n","\n","2. **Uwarunkowanie macierzy** $\\frac{\\lambda_{\\text{max}}(D_X)}{\\lambda_{\\text{min}}(D_X)}$ - duże $\\implies$ istnieje para predyktorów zależnych liniowo;\n","\n","3. **VIF** (ang. *variance inflation factor*) - współczynnik podbicia wariancji\n","\n","    Dla $1\\leq i \\leq p-1$: $$R^2_i = \\frac{\\text{RSS}}{\\text{TSS}}$$ dla modelu $x_i \\sim x_{-i}$, gdzie $x_{-i}$ oznacza wszystkie zmienne objaśniające z  pominięciem $i$-tej.\n","\n","    Wówczas\n","    $$\n","    \\text{VIF}_i = \\frac{1}{1-R_i^2}\n","    $$\n","\n","    **Interpretacja:** Duża wartość dla pewnego $i$ wskazuje na potencjalną liniową zależność $i$-tej zmiennej objaśniającej od pozostałych zmiennych. \n","\n","    **Reguła kciuka:** Jeśli $\\text{VIF}_i\\geq 10$, to $i$-tą zmienną uznajemy w przybliżeniu liniowo zależną od pozostałych."]},{"cell_type":"markdown","metadata":{"id":"ldihMp89QbVE"},"source":["# Zadanie 1\n","Dla danych `Carseats` sprawdź, czy występuje w nich problem współliniowości przy użyciu powyższych metod. Jeśli tak, odrzuć ze zbioru zmienne zależne liniowo i dopasuj model regresji liniowej bez nich. Porównaj wyniki."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"DYuM4Xz0QbVE"},"outputs":[],"source":["# carseats = sm.datasets.get_rdataset(dataname=\"Carseats\", package=\"ISLR\", cache=True)"]},{"cell_type":"code","source":["# Xvar = carseats.data.loc[:,~carseats.data.columns.isin(['Sales'])]\n","# features = Xvar.columns\n","# correlation_matrix = Xvar.corr()\n","# print(correlation_matrix)"],"metadata":{"id":"MNf3iBEYRhul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ##zrobimy heatmapę od wartości bezwzględnych 0 - brak koralcji,  1-mocna korelacja\n","# import matplotlib as mpl\n","# heatmap = plt.pcolor(np.abs(correlation_matrix), cmap=mpl.cm.coolwarm, alpha=0.8)\n","\n","# heatmap.axes.set_frame_on(False)\n","# heatmap.axes.set_yticks(np.arange(correlation_matrix.shape[0]) + 0.5, minor=False)\n","# heatmap.axes.set_xticks(np.arange(correlation_matrix.shape[1]) + 0.5, minor=False)\n","# heatmap.axes.set_xticklabels(features, minor=False)\n","# plt.xticks(rotation=90)\n","# heatmap.axes.set_yticklabels(features, minor=False)\n","# plt.tick_params(axis='both', which='both', bottom='off', \n","#                     top='off', left='off', right='off')\n","# plt.colorbar()\n","# plt.show()"],"metadata":{"id":"dKjIgrz7Rl25"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# #Vify\n","# from statsmodels.stats.outliers_influence import variance_inflation_factor\n","# columns = list(carseats.data.columns)\n","# columns.remove('Sales')\n","# features = \"+\".join(columns)\n","# y, X = dmatrices('Sales ~ ' + features, data=carseats.data, return_type='dataframe')\n","\n","# vif = pd.DataFrame()\n","# vif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n","# vif[\"features\"] = X.columns\n","# vif #tu dla wszystkich zmiennych jest ok"],"metadata":{"id":"9lA3H_aBRoKI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"w4lwRB6vQbVI"},"source":["# Zadanie 2\n","Wczytaj dane `kc_house_data.csv` ([This dataset contains house sale prices for King County, which includes Seattle. It includes homes sold between May 2014 and May 2015](https://www.kaggle.com/harlfoxem/housesalesprediction/data)).\n","\n","Dopasuj model `price ~ bathrooms + sqft_living + sqft_lot + sqft_above + sqft_basement + lat + long`, uwzględnij współliniowość predyktorów."]},{"cell_type":"code","source":["# house = pd.read_csv('kc_house_data.csv')\n","# house.head()"],"metadata":{"id":"adgVeXbvRsFE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# columns = np.array(['bathrooms', 'sqft_living', 'sqft_lot',\n","#                     'sqft_above', 'sqft_basement', 'lat', 'long'])\n","# lm = smf.ols('price~bathrooms+sqft_living+sqft_lot+sqft_above+sqft_basement+lat+long',data = house).fit()"],"metadata":{"id":"ZjED21XCmXD0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"mwFEd3TRQhmY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jh1bhUQJQbVK"},"source":["# Zadanie 3\n","Wczytaj zbiór `Hald.csv`. Znajdź najlepszy model regresji liniowej uwzględniając współliniowość predyktorów.\n","\n","Opis zbioru:\n","\n","    Heat evolved during setting of 13 cement mixtures of four basic ingredients. Each ingredient percentage appears to be rounded down to a full integer. The sum of the four mixture percentages varies from a maximum of 99% to a minimum of 95%. If all four regressor X-variables always summed to 100%, the centered X-matrix would then be of rank only 3. Thus, the regression of heat on four X-percentages is ill-conditioned, with an approximate rank deficiency of MCAL = 1. The first column is the response and the remaining four columns are the predictors."]},{"cell_type":"code","source":["# hald = pd.read_csv(\"Hald.csv\")\n","# hald.head()"],"metadata":{"id":"nvgN63LUQlL_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MFrrx4Z_QbVM"},"source":["# Obserwacje odstające"]},{"cell_type":"markdown","metadata":{"id":"NOcJ0JWHQbVM"},"source":["### Diagnostyka\n","\n","Obserwacja odstająca (ang.outlier) jest obserwacją, która nie spełnia równania regresji czyli nie należy do modelu regresji. Obserwacje odstające mogą znacząco wpływać na postać prostej regresji.\n","\n","**Rezyduum** $e_i$ przyjmuje dla $i$-tej obserwacji wartość różnicy:\n","$$\n","e_i = y_i - \\hat{y}_i.\n","$$\n","\n","**Błąd standardowy** takiego rezyduum $e_i$ jest równy:\n","$$\n","\\text{SE}(e_i) = S\\cdot\\sqrt{1-h_i},\n","$$\n","gdzie \n","- $S = \\sigma$ oznacza przęciętne odchylenie wartości rzeczywistych od wartości przewidywanych,\n","- $h_i$ - wartość wpływu $i$-tej obserwacji, która wyraża się wzorem\n","$$\n","h_i = \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{\\sum_{i=1}^n(x_i - \\overline{x})^2}\n","$$\n","\n","Obserwacje odstajacę dzielimy na \n","    - wpływowe - obserwacja jest wpływowa jesli jej usuniecie z modelu ma duży wpływ na dopasowanie modelu/prognoże na podstawie modelu;\n","    - niewpływowe - obserwacja jest niewpływowa jesli jej usuniecie z modelu nie ma wpływu na dopasowanie modelu/prognoże na podstawie modelu;\n","  "]},{"cell_type":"markdown","metadata":{"id":"sNnQ7otuQbVN"},"source":["  \n","### Detekcja obserwacji odstających:\n","\n","1. **Wykres studentyzowanych rezyduów**\n","\n","Dla małych prób, wartości zmiennej objaśniającej nie są w miarę równomiernie rozłożone i niektóre błędy $\\text{SE}(e_i)$ mogą znacznie odbierać od błędu $S$. Wówczas dobrze jest analizować rezydua przy użyciu tzw. **rezyduów studentyzowanych**.\n","\n","$$r_i =\\frac{e_i}{\\text{SE}(e_i)}$$\n","\n","To pozwoli wykrywać obserwacje faktycznie odstające, pomijając te, które przy analizie rezyduów $e_i$ sugerowały, że są odstające mimo, że takimi nie były. Dla rezyduów studentyzowanych zakłada się, że przy poziomie ufności równym 0.95 uznaje się je za normalne (zachowujące własność rozkładu normalnego), gdy należą do przedziału $[−2,+2]$.\n","\n","Wykres studentyzowanych rezyduów względem ich indeksu identyfikuje duże wartości, które przypuszczalnie odpowiadają obserwacjom odstającym. Metodata nie sprawdzi się w sytuacji, gdy mamy w analizowanym zbiorze obserwację wpływową o małej wartości $e_i$. Wówczas bowiem nie określimy jej jako odstającej mimo, że taka w istocie jest."]},{"cell_type":"markdown","metadata":{"id":"fBDUcQ59QbVP"},"source":["2. **Wpływowość**\n","\n","Wpływ $i$-tej obserwacji $h_i$ określamy wzorem\n","$$\n","h_i = \\frac{1}{n} + \\frac{(x_i - \\overline{x})^2}{\\sum_{i=1}^n(x_i - \\overline{x})^2},\n","$$ \n","który określa odstępstwo $x_i$ od $\\overline{x}$.\n","\n","Dla modelu o $p$ parametrach (gdzie $p$ to łączna liczba zmiennych objaśniających i objaśnianych), obserwację uznajemy za wpływową jeśli \n","$$\n","h_i \\geq \\frac{2p}{n}.\n","$$"]},{"cell_type":"markdown","metadata":{"id":"FWh3TKXPQbVP"},"source":["3. **Odległość Cooka**\n","\n","Jest to miara stopnia zmiany współczynników regresji, gdyby dany przypadek pominąć w obliczeniach współczynników:\n","$$\n","D_i = \\frac{\\sum_{j=1}^n(\\hat{Y}_j - \\hat{Y}_{j(i)})^2}{pS^2},\n","$$\n","gdzie $\\hat{Y}_j$ - prognoza na podstawie pełnych danych, $\\hat{Y}_{j(i)}$ - prognoza bez $i$-tej obserwacji.\n","\n","**Interpretacja**: Duża wartość $D_i$ wskazuje na znaczy wpływ usunięcia $i$-tej obserwacji, czyli $i$-ta obserwacja jest obserwacją wpływową.\n","\n","Wszystkie wartości dla danej odległości powinny być tego samego rzędu. Jeśli tak nie jest, to prawdopodobnie dany przypadek ma istotnie duży wpływ na obciążenie równania regresji.\n","\n","**Reguła kciuka**: $D_i > \\frac{4}{(n − p − 1)}$"]},{"cell_type":"markdown","metadata":{"id":"9xiJa-NJQbVQ"},"source":["# Zadanie 4\n","Przeanalizuj obserwacje odstające w modelu `model` dla danych `Carseats`. Zidentyfikuj obserwacje im odpowiadające, usuń je ze zbioru i zbuduj model ponownie. Porównaj dopasowanie modeli. \n","Analogicznie postępuj dla modelu `model2`."]},{"cell_type":"code","source":["# carseats = sm.datasets.get_rdataset(dataname=\"Carseats\", package=\"ISLR\", cache=True)"],"metadata":{"id":"HBNkxZ-6c8Bc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# carseats_df = carseats.data"],"metadata":{"id":"3M4PeuSOLkxm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# columns = list(carseats_df.columns)\n","# columns.remove('Sales')\n","# features = \"+\".join(columns)\n","# print(features)\n","# model = smf.ols('Sales~'+features,data = carseats_df)\n","# fitted = model.fit()\n","# fitted.summary()"],"metadata":{"id":"R44hKaCuQ5CO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#h = fitted.get_influence()\n","#h.resid_studentized"],"metadata":{"id":"Wk-rUMxRQ1Sm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ##wartości odstające\n","# x = np.arange(1, h.resid_studentized.size + 1)\n","# plt.scatter(x=x, y=h.resid_studentized)\n","# plt.hlines(xmin=1, xmax=h.resid_studentized.size + 1, y=-2, color=\"r\")\n","# plt.hlines(xmin=1, xmax=h.resid_studentized.size + 1, y=0, color=\"r\")\n","# plt.hlines(xmin=1, xmax=h.resid_studentized.size + 1, y=2, color=\"r\")\n","# ##te poza pasem kwalifikujemy jako do usunięcia"],"metadata":{"id":"4eTzwMQiQxwK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ##wartości wpływowe h_i\n","# h.hat_matrix_diag\n","# threshold = 2*h.exog.shape[1]/h.exog.shape[0]\n","\n","# x = np.arange(1, h.hat_matrix_diag.size + 1)\n","# plt.scatter(x=x, y=h.hat_matrix_diag)\n","# plt.hlines(xmin=1, xmax=h.hat_matrix_diag.size + 1, y=threshold, color=\"r\")\n","# ##są dwie obs wpływowe"],"metadata":{"id":"Es2ojRB4QvyX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ##obserwacje wpływowe odległości cooka (inna metoda detekcji wpływowości)\n","# threshold = 4/(h.exog.shape[0]-h.exog.shape[1]-1)\n","# h.cooks_distance\n","# x = np.arange(1, h.cooks_distance[0].size + 1)\n","# plt.scatter(x=x, y=h.cooks_distance[0])\n","# plt.hlines(xmin=1, xmax = x[-1], y = threshold, color='r')"],"metadata":{"id":"7zH0OLeUQsle"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}